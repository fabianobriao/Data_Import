{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "dataimport.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "mount_file_id": "1v0tiAARCXDdNf25ed1hGAPHrIBNaySkW",
      "authorship_tag": "ABX9TyNeE5tuQx3b0RBy1MxgUruw",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/fabianobriao/Data_Import_Guide_for_Beginners/blob/main/dataimport.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KnIjbR7aB9sx"
      },
      "source": [
        "## Este repositório é um guia para iniciantes em Análise e Ciência de Dados, exclusivo para mostrar processos básicos de preparação de conjuntos de dados através de técnicas de reconhecimento de outliers, dados ausentes, nulos, não reconhecidos, dados homógrafos, homônimos perfeitos e sinônimos. Assim como técnicas de correções de cabeçalhos, índices e padronização de separadores, entre outras. As importações dos dados podem ser observadas com detalhes no repositório Data_Import.\n",
        "\n",
        "## Os scripts foram construídos na ide PyCharm, por isso algumas sintaxes possam parecer diferentes das usuais para usuários mais habituados ao Google Colaboratory."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tWu3iBo-sdpv"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import xlrd\n",
        "import json\n",
        "!pip install geopandas\n",
        "import geopandas as gpd\n",
        "import gspread"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V8v896M6vn9q"
      },
      "source": [
        "## CSV"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fQGRdE4wuc4O",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3c3a04a4-66e2-4bee-e0a5-222f54f9901d"
      },
      "source": [
        "## importe de arquivos csv (1º caso)\n",
        "file = \"/content/drive/MyDrive/Colab Notebooks/Data_Import/Resp2.csv\"\n",
        "df1 = pd.read_csv(file)\n",
        "print(df1.head(3))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "   experience  respiration\n",
            "0           0         3.94\n",
            "1           0         4.26\n",
            "2           0         4.16\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_d9af0_-v1Rw",
        "outputId": "90e9bcf1-c8f5-495e-a201-8b4bbe10f74b"
      },
      "source": [
        "## importe de arquivos csv (2º caso, arquivo separado por ;)\n",
        "file = \"/content/drive/MyDrive/Colab_Notebooks/Data_Import/Churn.csv\"\n",
        "df2 = pd.read_csv(file, sep = \";\")\n",
        "print(df2.head(3))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "   X0   X1  X2        X3  X4  X4.1       X6  X7  X8  X9         X10  X11\n",
            "0   1  619  RS  Feminino  42     2        0   1   1   1  10134888.0    1\n",
            "1   2  608  SC  Feminino  41     1  8380786   1   0   1  11254258.0    0\n",
            "2   3  502  RS  Feminino  42     8  1596608   3   1   0  11393157.0    1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A_kDLMCjNV32"
      },
      "source": [
        "## XLS"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HpCfDrdOvWVW",
        "outputId": "5c201b56-b710-4de0-f9cf-544be467a4b9"
      },
      "source": [
        "## importe de arquivos xls\n",
        "file = \"/content/drive/MyDrive/Colab Notebooks/Data_Import/boston1.xls\"\n",
        "df2 = pd.ExcelFile(file) #tem que instalar o xlrd\n",
        "print(df2.sheet_names)\n",
        "df2 = df2.parse('Sheet2')\n",
        "print(df2.head(3))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['Sheet1', 'Sheet2']\n",
            "     MV  INDUS   NOX     RM  TAX    PT  LSTAT\n",
            "0  24.0   2.31  53.8  6.575  296  15.3   4.98\n",
            "1  21.6   7.07  46.9  6.421  242  17.8   9.14\n",
            "2  34.7   7.07  46.9  7.185  242  17.8   4.03\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bl9YR3BmNRGR"
      },
      "source": [
        "## XLSX"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rLwfrEiavVjg",
        "outputId": "8b1ac605-564b-4e6b-e67c-59fd1b514b69"
      },
      "source": [
        "## importe de arquivos xlsx # tem que instalar o xlrd\n",
        "#import xlrd\n",
        "file = \"/content/drive/MyDrive/Colab Notebooks/Data_Import/boston11.xlsx\"\n",
        "df2 = pd.ExcelFile(file) \n",
        "print(df2.sheet_names)\n",
        "df2 = df2.parse('Planilha3')\n",
        "print(df2.head(5))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['planilha1', 'planilha2', 'Planilha3']\n",
            "     MV  INDUS\n",
            "0  24.0   2.31\n",
            "1  21.6   7.07\n",
            "2  34.7   7.07\n",
            "3  33.4   2.18\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eE-AlkK21Zv6",
        "outputId": "b6e40c59-d9c0-48f3-f9e2-7ede74674ffe"
      },
      "source": [
        "## importe de planilha da internet (no pycharm é necessário instalar lxml)\n",
        "site = \"https://pt.wikipedia.org/wiki/Unidades_federativas_do_Brasil\" # tem que importar html5lib\n",
        "br = pd.read_html(site)\n",
        "#print(type(br))\n",
        "print(br[0].head(5)) # havendo mais de uma tabela puxamos pelo índice"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              0                                       1           2\n",
            "0    Capítulo I  Da Organização Político-Administrativa  art. 18-19\n",
            "1   Capítulo II                                Da União  art. 20-24\n",
            "2  Capítulo III                   Dos Estados Federados  art. 25-28\n",
            "3   Capítulo IV                          Dos Municípios  art. 29-31\n",
            "4    Capítulo V   Do Distrito Federal e dos Territórios  art. 32-33\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H7wpTDW7NJHV"
      },
      "source": [
        "## JSON"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R3ARvsi82dEB",
        "outputId": "d9be23e9-22f7-4949-d18b-5f74c121c8c0"
      },
      "source": [
        "# importe de arquivos json\n",
        "# import json\n",
        "file = \"/content/drive/MyDrive/Colab Notebooks/Data_Import/skorea.json\"\n",
        "with open(file) as j:\n",
        "    Json = json.load(j)\n",
        "# print(Json)\n",
        "df3 = pd.DataFrame(Json)\n",
        "print(df3.head(4))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  Description  ...  Year\n",
            "0              ...  2015\n",
            "1              ...  2000\n",
            "2              ...  1997\n",
            "3              ...  2000\n",
            "\n",
            "[4 rows x 7 columns]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qi8KxEDv59_3",
        "outputId": "c0c3e102-7be8-48ac-89a3-53bc2d2575c7"
      },
      "source": [
        "# importe de arquivos json - amostra de dados extraída de dehttps://servicodados.ibge.gov.br/api/v1/localidades/distritos\n",
        "# import json\n",
        "file = \"/content/drive/MyDrive/Colab Notebooks/Data_Import/municipio.json\"\n",
        "with open(file) as j:\n",
        "    Json = json.load(j)\n",
        "# print(Json)\n",
        "df4 = pd.DataFrame(Json)\n",
        "print(df4.head(4))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "          id  ...                                          municipio\n",
            "0  520005005  ...  {'id': 5200050, 'nome': 'Abadia de Goiás', 'mi...\n",
            "1  310010405  ...  {'id': 3100104, 'nome': 'Abadia dos Dourados',...\n",
            "2  520010005  ...  {'id': 5200100, 'nome': 'Abadiânia', 'microrre...\n",
            "3  520010010  ...  {'id': 5200100, 'nome': 'Abadiânia', 'microrre...\n",
            "\n",
            "[4 rows x 3 columns]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ixs1QVDINDUq"
      },
      "source": [
        "## SHP"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fkBHF1q6CMzK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ea288911-7036-45fe-9995-e4866f35f296"
      },
      "source": [
        "# Importe de arquivos shp - precisamos instalar o geopandas\n",
        "# import geopandas as gpd\n",
        "file = \"/content/drive/MyDrive/Colab Notebooks/Data_Import/AL_Municipios_2020/AL_Municipios_2020.shp\"\n",
        "datashp = gpd.read_file(file)\n",
        "print(datashp.head(4))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "    CD_MUN  ...                                           geometry\n",
            "0  2700102  ...  POLYGON ((-37.95164 -9.09743, -37.88934 -9.162...\n",
            "1  2700201  ...  POLYGON ((-36.34926 -9.59310, -36.34920 -9.593...\n",
            "2  2700300  ...  POLYGON ((-36.66005 -9.64201, -36.66002 -9.642...\n",
            "3  2700409  ...  POLYGON ((-35.92889 -9.37557, -35.93043 -9.441...\n",
            "\n",
            "[4 rows x 5 columns]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P4TS4LzVQrfG"
      },
      "source": [
        "## *GSPREASHEET*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3-iWuXVe4fWQ"
      },
      "source": [
        "Neste tópico iremos baixar um planilha do google drive, modificá-la e atualizá-la.\n",
        "Também iremos criar uma nova planilha para adicionar ao arquivo das planilhas já existentes."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XCNrdsh1Pt4p"
      },
      "source": [
        "from google.colab import auth\n",
        "auth.authenticate_user()\n",
        "#import gspread\n",
        "from oauth2client.client import GoogleCredentials\n",
        "gc = gspread.authorize(GoogleCredentials.get_application_default())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YC62vESPUX_A"
      },
      "source": [
        "planilhagg = gc.open('boston1GSheets') #planilha existente no Google Drive"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9dCpaaA_0g3C"
      },
      "source": [
        "planilha1 = planilhagg.sheet1 # sheet1 carrega a planilha 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "id": "RVRmcmmt2Wdq",
        "outputId": "e0fe665d-1eb8-4d5e-b409-ea685f1656dc"
      },
      "source": [
        "#transformar a planilha 1 importada em dataframe\n",
        "boston1GSheetspl1 = pd.DataFrame(planilha1.get_all_records())\n",
        "boston1GSheetspl1.head(5)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>MV</th>\n",
              "      <th>INDUS</th>\n",
              "      <th>NOX</th>\n",
              "      <th>RM</th>\n",
              "      <th>TAX</th>\n",
              "      <th>PT</th>\n",
              "      <th>LSTAT</th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>24</td>\n",
              "      <td>2,31</td>\n",
              "      <td>53,8</td>\n",
              "      <td>6,575</td>\n",
              "      <td>296</td>\n",
              "      <td>15,3</td>\n",
              "      <td>4,98</td>\n",
              "      <td>Subset of Boston housing tract</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>21,6</td>\n",
              "      <td>7,07</td>\n",
              "      <td>46,9</td>\n",
              "      <td>6,421</td>\n",
              "      <td>242</td>\n",
              "      <td>17,8</td>\n",
              "      <td>9,14</td>\n",
              "      <td>data of Harrison and Rubinfeld</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>34,7</td>\n",
              "      <td>7,07</td>\n",
              "      <td>46,9</td>\n",
              "      <td>7,185</td>\n",
              "      <td>242</td>\n",
              "      <td>17,8</td>\n",
              "      <td>4,03</td>\n",
              "      <td>(1978).  Each case is one U.S.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>33,4</td>\n",
              "      <td>2,18</td>\n",
              "      <td>45,8</td>\n",
              "      <td>6,998</td>\n",
              "      <td>222</td>\n",
              "      <td>18,7</td>\n",
              "      <td>2,94</td>\n",
              "      <td>Census tract in the Boston area.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>36,2</td>\n",
              "      <td>2,18</td>\n",
              "      <td>45,8</td>\n",
              "      <td>7,147</td>\n",
              "      <td>222</td>\n",
              "      <td>18,7</td>\n",
              "      <td>5,33</td>\n",
              "      <td></td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "     MV INDUS   NOX     RM  TAX    PT LSTAT                                  \n",
              "0    24  2,31  53,8  6,575  296  15,3  4,98    Subset of Boston housing tract\n",
              "1  21,6  7,07  46,9  6,421  242  17,8  9,14    data of Harrison and Rubinfeld\n",
              "2  34,7  7,07  46,9  7,185  242  17,8  4,03    (1978).  Each case is one U.S.\n",
              "3  33,4  2,18  45,8  6,998  222  18,7  2,94  Census tract in the Boston area.\n",
              "4  36,2  2,18  45,8  7,147  222  18,7  5,33                                  "
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 226
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "id": "eH2PgtRl6NAH",
        "outputId": "ca21935f-ed76-4f30-85d5-1f64b11081ef"
      },
      "source": [
        "type(boston1GSheetspl1)\n",
        "#boston1GSheetspl1.info()\n",
        "planilha1nova = boston1GSheetspl1.query('TAX>=200') #criando a nova planilha\n",
        "planilha1nova.head(5)\n",
        "#planilha1nova.info() #489linhas e 8 colunas"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>MV</th>\n",
              "      <th>INDUS</th>\n",
              "      <th>NOX</th>\n",
              "      <th>RM</th>\n",
              "      <th>TAX</th>\n",
              "      <th>PT</th>\n",
              "      <th>LSTAT</th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>24</td>\n",
              "      <td>2,31</td>\n",
              "      <td>53,8</td>\n",
              "      <td>6,575</td>\n",
              "      <td>296</td>\n",
              "      <td>15,3</td>\n",
              "      <td>4,98</td>\n",
              "      <td>Subset of Boston housing tract</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>21,6</td>\n",
              "      <td>7,07</td>\n",
              "      <td>46,9</td>\n",
              "      <td>6,421</td>\n",
              "      <td>242</td>\n",
              "      <td>17,8</td>\n",
              "      <td>9,14</td>\n",
              "      <td>data of Harrison and Rubinfeld</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>34,7</td>\n",
              "      <td>7,07</td>\n",
              "      <td>46,9</td>\n",
              "      <td>7,185</td>\n",
              "      <td>242</td>\n",
              "      <td>17,8</td>\n",
              "      <td>4,03</td>\n",
              "      <td>(1978).  Each case is one U.S.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>33,4</td>\n",
              "      <td>2,18</td>\n",
              "      <td>45,8</td>\n",
              "      <td>6,998</td>\n",
              "      <td>222</td>\n",
              "      <td>18,7</td>\n",
              "      <td>2,94</td>\n",
              "      <td>Census tract in the Boston area.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>36,2</td>\n",
              "      <td>2,18</td>\n",
              "      <td>45,8</td>\n",
              "      <td>7,147</td>\n",
              "      <td>222</td>\n",
              "      <td>18,7</td>\n",
              "      <td>5,33</td>\n",
              "      <td></td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "     MV INDUS   NOX     RM  TAX    PT LSTAT                                  \n",
              "0    24  2,31  53,8  6,575  296  15,3  4,98    Subset of Boston housing tract\n",
              "1  21,6  7,07  46,9  6,421  242  17,8  9,14    data of Harrison and Rubinfeld\n",
              "2  34,7  7,07  46,9  7,185  242  17,8  4,03    (1978).  Each case is one U.S.\n",
              "3  33,4  2,18  45,8  6,998  222  18,7  2,94  Census tract in the Boston area.\n",
              "4  36,2  2,18  45,8  7,147  222  18,7  5,33                                  "
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 213
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MqASPystaHEn"
      },
      "source": [
        "planilha1nova1 = planilhagg.add_worksheet('planilha6', rows=489, cols=8) #criando a nova planilha no drive chamada planilha5"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3RqGJpCnaatK"
      },
      "source": [
        "planilha1nova1.update([planilha1nova.columns.values.tolist()] + planilha1nova.values.tolist()) #transformando dados numa lista de listas e carregando o google planilhas"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N3IBvyY9tReZ"
      },
      "source": [
        "# WordCloud"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BK7x4bvBnMrx"
      },
      "source": [
        "from newspaper import Article  # !pip install newspaper3k\n",
        "from PIL import Image\n",
        "from os import path\n",
        "from wordcloud import WordCloud,STOPWORDS, ImageColorGenerator # !pip install wordcloud\n",
        "#import matplotlib.pyplot as plt\n",
        "% matplotlib inline"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u6G0IxUp8iT3"
      },
      "source": [
        "Dados: Airbnb - RJ - 21/03/2021 - arquivo csv"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QwqVwhRS8kG9",
        "outputId": "f3aca736-b2f3-4636-82f8-be21a62c030c"
      },
      "source": [
        "# data frame já está apenas com a coluna summary, que será utilizada apenas nesse caso\n",
        "dfw = pd.read_csv(\"http://dl.dropboxusercontent.com/s/fdzigk974zbx2de/listings.csv?dl=0\")\n",
        "# eliminar as colunas com valores ausentes\n",
        "summary = df.dropna(subset=['summary'], axis=0)['summary']\n",
        "summary.head(3)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/IPython/core/interactiveshell.py:2718: DtypeWarning: Columns (61,62,94) have mixed types.Specify dtype option on import or set low_memory=False.\n",
            "  interactivity=interactivity, compiler=compiler, result=result)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0    Please note that special rates apply for Carni...\n",
              "1    Our apartment is a little gem, everyone loves ...\n",
              "2    This nice and clean 1 bedroom apartment is loc...\n",
              "Name: summary, dtype: object"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 190
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        },
        "id": "h8dHLdxR9VUn",
        "outputId": "eff9c6ae-cef4-4167-cfa3-1665c51be318"
      },
      "source": [
        "# exemplos de descrições para os imóveis no Airbnb\n",
        "display(summary.iloc[100])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Ambiente tranquilo e familiar,ótimo para viajantes que no final do dia queiram descansar e ter uma noite tranquila. Excelente bairro que fica próximo a Lagoa Rodrigo de Freitas, que é um dos lindos cartões postais de nossa cidade. Aguardo você!'"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UuegZdA--mK0",
        "outputId": "a8455e5f-1a1d-48b0-b1bd-55254dc8d6d0"
      },
      "source": [
        "# concatenando as palavras\n",
        "summarystr = \" \".join(s for s in summary)\n",
        "type(summarystr)\n",
        "print(summarystr)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "IOPub data rate exceeded.\n",
            "The notebook server will temporarily stop sending output\n",
            "to the client in order to avoid crashing it.\n",
            "To change this limit, set the config variable\n",
            "`--NotebookApp.iopub_data_rate_limit`.\n",
            "\n",
            "Current values:\n",
            "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
            "NotebookApp.rate_limit_window=3.0 (secs)\n",
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z7yXRiKl-8uh"
      },
      "source": [
        "# lista de stopword\n",
        "stopwords = set(STOPWORDS)\n",
        "stopwords.update(['da', 'meu', 'em', 'você', 'de', 'ao', 'os','ver','principal','essa','vez','nas','mas','qual','principal','ele','ter','doença','pois','este','vez','ver principal','artigo principal','já','aos','pode','outro','artigo','desse','alguns','meio','entre','das','podem','esse','seu','também','são','quando','de', 'que','em','os','as','da','como','dos','ou','se','um','uma','para','na','ao','mais','por','não','ainda','muito','sua'] )\n",
        "#Definindo a lista de stopwords\n",
        "stopwords= set(STOPWORDS)\n",
        "stopwords\n",
        "# Outra forma, importando um arquivoAdicionando a lista stopwords em português\n",
        "# new_words = []\n",
        "# file = \"/content/drive/MyDrive/Colab Notebooks/Data_Import/stopwords.txt\"\n",
        "# with open(file, 'r') as f:\n",
        "#     [new_words.append(word) for line in f for word in line.split()]\n",
        "# new_stopwords = stopwords.union(new_words)\n",
        "#new_stopwords"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hYOx3f-cBono"
      },
      "source": [
        "Dados: meu CV, arquivo pdf convertido em string"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VU_O4viyqIVx"
      },
      "source": [
        "import pdfminer #!pip install pdfminer.six\n",
        "import io\n",
        "from pdfminer.converter import TextConverter\n",
        "from pdfminer.pdfinterp import PDFPageInterpreter\n",
        "from pdfminer.pdfinterp import PDFResourceManager\n",
        "from pdfminer.pdfpage import PDFPage"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XQUrzkhFIv1z",
        "outputId": "5138fc72-5a4e-4cf8-b1f5-9cda245cd2d9"
      },
      "source": [
        "file = \"/content/drive/MyDrive/Colab Notebooks/Data_Import/CurriculoFabianoBriao_port_jun21.pdf\"\n",
        "#defining function \n",
        "#reference: http://www.blog.pythonlibrary.org/2018/05/03/exporting-data-from-pdfs-with-python/\n",
        "def extract_text_from_pdf(pdf_path):\n",
        "    resource_manager = PDFResourceManager()\n",
        "    fake_file_handle = io.StringIO()\n",
        "    converter = TextConverter(resource_manager, fake_file_handle)\n",
        "    page_interpreter = PDFPageInterpreter(resource_manager, converter)\n",
        "    with open(pdf_path, 'rb') as fh:\n",
        "        for page in PDFPage.get_pages(fh, \n",
        "                                      caching=True,\n",
        "                                      check_extractable=True):\n",
        "            page_interpreter.process_page(page)\n",
        "        text = fake_file_handle.getvalue()\n",
        "    # close open handles\n",
        "    converter.close()\n",
        "    fake_file_handle.close()\n",
        "    if text:\n",
        "        return text\n",
        "\n",
        "#executing function and printing text\n",
        "CVstr = extract_text_from_pdf(file)\n",
        "type(CVstr)\n",
        "print(CVstr)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "RESUMOCientista de Dados com expertise em inteligência artificial e largaexperiência como Analista de Dados/Estatístico, presente no mercado detecnologia desde 2010.Atuo como analista de dados freelancer por uma década, além de realizartrabalhos como cientista de dados. Nessa jornada adquiri experiência ehabilidades como comunicação, proatividade, autonomia, pensamento crítico enegociação.Utilizo técnicas de ETL(extração, transformação, carregamento),apresentação, interpretação, análise de dados, modelagem, desenvolvimento etestes de validação com utilização de R, Python, Tableau Prep e Google Studio.Além, de desenvolvimento em projetos de machine learning.Trabalho com bancos de dados em vários SGBD com conhecimento delinguagem avançado. E utilizo de ferramentas de visualização como Tableau eGoogle Analytics para gráficos, dashboards e apresentações.Tenho um histórico relacionado à gestão que me conforta para liderarequipes e tomar decisões. Isso se deve aos 15 anos de experiência comoprofessor universitário ministrando aulas em cursos de engenharia eadministração com passagens em cargos de coordenador e de colegiado.FERRAMENTAS E HABILIDADESLinguagens de programação: Python avançado, R avançado, SQLavançado.Visualização de dados: Tableau Desktop avançado, Google Analytics.VCS: Git, GitHub.Database: MySQL, PostgreSQL, MongoDB, SQL Server, Google BigQuery.Plataformas: Google Colab, Jupyter Notebook, Tableau Prep…Bibliotecas: Pandas, numpy, igraph, lattice, seaborn, matplotlib, ...Obs.: Tenho conhecimento de Google Cloud Platform(GCP) e cursos deespecialização em rede, engenharia e arquitetura com o Google Compute Engine.Hoje em dia, sou estudante avançado de inglês, com foco em conversação.EXPERIÊNCIA PROFISSIONALCientista de DadosCoding TecnologiaDezembro de 2020 – Atual.Atuo como cientista de dados freelancer sob demanda de projetos emlinguagem Python e R, com expertise em Machine Learning. Analista de DadosCoding TecnologiaJaneiro de 2019 – Dezembro de 2020(2 anos).Atuei como analista de dados freelancer onde realizava processos de ETLcom desenvolvimento de projetos em linguagem R. Relatórios e dashboardsque usualmente aconteciam por Tableau. Este trabalho em sua grandeparte servia à demandas acadêmicas.Fabiano dos Santos BriãoCientista de dados | Analista de Dados | Estatístico Fone: +55 82 999248968E-mail: fabianobriao@gmail.comPersonal page : http://fabianobriao.github.ioLinkedIn: https://www.linkedin.com/in/fabianobriao\fCientista de Dados – Pesquisador de Análises Ambientais IIFundação Espírito-Santense de Tecnologia(FEST).Julho de 2019 – Dezembro de 2019 (6 meses).Trabalhei como Cientista de Dados analisando aves marinhas em váriosaspectos como vôo, mergulho e alimentação. Com dados obtidos porsensores presos ao corpo dessas aves. Um projeto que avaliou o impactoambiental causado pela queda da barragem de Mariana em Minas Gerais.Neste projeto, usamos vários métodos de estatística descritiva e inferencial,séries temporais e machine learning.ProfessorInstituto Federal de Educação, Ciência e Tecnologia de Alagoas(IFAL/AL). Agosto de 2019 – Atual.Leciono para cursos de engenharia e Sistemas de informação.Professor Adjunto ICentro Universitário Tiradentes(Unit).Janeiro de 2013 – Julho de 2019 (6 anos e 7 meses).Lecionei para cursos de engenharia, Administração de Empresas e Ciênciada Computação. Neste período atuei como membro de colegiado de cursos.ProfessorCentro Universitário Cesmac.Setembro de 2007 – Fevereiro de 2011 (5 anos e 6 meses).Lecionei para cursos de engenharia e Análise de Sistemas. Sou autor docurso de Especialização em Matemática e fui coordenador.FORMAÇÃO ACADÊMICA/TITULAÇÃOMestrado em Modelagem Computacional de ConhecimentoUniversidade Federal de Alagoas(UFAL), Maceió/AL, Brasil.Licenciatura Plena em MatemáticaUniversidade Federal de Rio Grande(FURG), Rio Grande/RS, Brasil.IDIOMASPortuguês nativo / Inglês avançado.CERTIFICAÇÕESHuawei Certification: HCIA-5GHuawei Certification: HCIA-AICURSOS/CERTIFICADOSPython e R: Analytics e DadosCrash Course on PythonPython and Statistics for Financial AnalysisSéries Temporais e Análises Preditivas: O Curso CompletoTableau Desktop AvançadoCurso Completo Tableau PrepLaunching into Machine LearningHow Google does Machine LearningDocker para Desenvolvedores (com Docker Swarm e Kubernetes)Git e GitHub do básico ao avançado (c/ gist e GitHub Pages)Google Cloud Product FundamentalsOutros cursos e mais informações estão disponíveis no LinkedIn.\f\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DEwmcz1wqU3j"
      },
      "source": [
        "Dados: meu CV, arquivo docx convertido em string"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 171
        },
        "id": "mdtOjwuuqa7N",
        "outputId": "009427f8-a0c3-49e0-edc2-350c1a130fdf"
      },
      "source": [
        "import pypandoc #!pip install pypandoc\n",
        "file = \"/content/drive/MyDrive/Colab Notebooks/Data_Import/CurriculoFabianoBriao_port_jun21.docx\"\n",
        "\n",
        "CVstr = (pypandoc.convert_file(file, \"plain+simple_tables\", format=\"docx\", extra_args=(), encoding='utf-8', outputfile=None))\n",
        "CVstr"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'RESUMO\\n\\nCientista de Dados com expertise em inteligência artificial e larga\\nexperiência como Analista de Dados/Estatístico, presente no mercado de\\ntecnologia desde 2010.\\n\\nAtuo como analista de dados freelancer por uma década, além de realizar\\ntrabalhos como cientista de dados. Nessa jornada adquiri experiência e\\nhabilidades como comunicação, proatividade, autonomia, pensamento\\ncrítico e negociação.\\n\\nUtilizo técnicas de ETL(extração, transformação, carregamento),\\napresentação, interpretação, análise de dados, modelagem,\\ndesenvolvimento e testes de validação com utilização de R, Python,\\nTableau Prep e Google Studio. Além, de desenvolvimento em projetos de\\nmachine learning.\\n\\nTrabalho com bancos de dados em vários SGBD com conhecimento de\\nlinguagem avançado. E utilizo de ferramentas de visualização como\\nTableau e Google Analytics para gráficos, dashboards e apresentações.\\n\\nTenho um histórico relacionado à gestão que me conforta para liderar\\nequipes e tomar decisões. Isso se deve aos 15 anos de experiência como\\nprofessor universitário ministrando aulas em cursos de engenharia e\\nadministração com passagens em cargos de coordenador e de colegiado.\\n\\nFERRAMENTAS E HABILIDADES\\n\\n  LINGUAGENS DE PROGRAMAÇÃO: Python avançado, R avançado, SQL avançado.\\n\\nVISUALIZAÇÃO DE DADOS: Tableau Desktop avançado, Google Analytics.\\n\\nVCS: Git, GitHub.\\n\\nDATABASE: MySQL, PostgreSQL, MongoDB, SQL Server, Google BigQuery.\\n\\nPLATAFORMAS: Google Colab, Jupyter Notebook, Tableau Prep…\\n\\nBIBLIOTECAS: Pandas, numpy, igraph, lattice, seaborn, matplotlib, ...\\n\\nObs.: Tenho conhecimento de Google Cloud Platform(GCP) e cursos de\\nespecialização em rede, engenharia e arquitetura com o Google Compute\\nEngine. Hoje em dia, sou estudante avançado de inglês, com foco em\\nconversação.\\n\\nEXPERIÊNCIA PROFISSIONAL\\n\\nCIENTISTA DE DADOS\\n\\nCoding Tecnologia\\n\\nDezembro de 2020 – Atual.\\n\\n  Atuo como cientista de dados freelancer sob demanda de projetos em\\n  linguagem Python e R, com expertise em Machine Learning.\\n\\nANALISTA DE DADOS\\n\\nCoding Tecnologia\\n\\nJaneiro de 2019 – Dezembro de 2020(2 anos).\\n\\n  Atuei como analista de dados freelancer onde realizava processos de\\n  ETL com desenvolvimento de projetos em linguagem R. Relatórios e\\n  dashboards que usualmente aconteciam por Tableau. Este trabalho em sua\\n  grande parte servia à demandas acadêmicas.\\n\\nCIENTISTA DE DADOS – PESQUISADOR DE ANÁLISES AMBIENTAIS II\\n\\nFundação Espírito-Santense de Tecnologia(FEST).\\n\\nJulho de 2019 – Dezembro de 2019 (6 meses).\\n\\n  Trabalhei como Cientista de Dados analisando aves marinhas em vários\\n  aspectos como vôo, mergulho e alimentação. Com dados obtidos por\\n  sensores presos ao corpo dessas aves. Um projeto que avaliou o impacto\\n  ambiental causado pela queda da barragem de Mariana em Minas Gerais.\\n  Neste projeto, usamos vários métodos de estatística descritiva e\\n  inferencial, séries temporais e machine learning.\\n\\nPROFESSOR\\n\\nInstituto Federal de Educação, Ciência e Tecnologia de Alagoas(IFAL/AL).\\n\\nAgosto de 2019 – Atual.\\n\\nLeciono para cursos de engenharia e Sistemas de informação.\\n\\nPROFESSOR ADJUNTO I\\n\\nCentro Universitário Tiradentes(Unit).\\n\\nJaneiro de 2013 – Julho de 2019 (6 anos e 7 meses).\\n\\n  Lecionei para cursos de engenharia, Administração de Empresas e\\n  Ciência da Computação. Neste período atuei como membro de colegiado de\\n  cursos.\\n\\nPROFESSOR\\n\\nCentro Universitário Cesmac.\\n\\nSetembro de 2007 – Fevereiro de 2011 (5 anos e 6 meses).\\n\\n  Lecionei para cursos de engenharia e Análise de Sistemas. Sou autor do\\n  curso de Especialização em Matemática e fui coordenador.\\n\\nFORMAÇÃO ACADÊMICA/TITULAÇÃO\\n\\nMESTRADO EM MODELAGEM COMPUTACIONAL DE CONHECIMENTO\\n\\nUniversidade Federal de Alagoas(UFAL), Maceió/AL, Brasil.\\n\\nLICENCIATURA PLENA EM MATEMÁTICA\\n\\nUniversidade Federal de Rio Grande(FURG), Rio Grande/RS, Brasil.\\n\\nIDIOMAS\\n\\nPortuguês nativo / Inglês avançado.\\n\\nCERTIFICAÇÕES\\n\\n_Huawei Certification: HCIA-5G_\\n\\n_Huawei Certification: HCIA-AI_\\n\\nCURSOS/CERTIFICADOS\\n\\n_Python e R: Analytics e Dados_\\n\\n_Crash Course on Python_\\n\\n_Python and Statistics for Financial Analysis_\\n\\n_Séries Temporais e Análises Preditivas: O Curso Completo_\\n\\n_Tableau Desktop Avançado_\\n\\n_Curso Completo Tableau Prep_\\n\\n_Launching into Machine Learning_\\n\\n_How Google does Machine Learning_\\n\\n_Docker para Desenvolvedores (com Docker Swarm e Kubernetes)_\\n\\n_Git e GitHub do básico ao avançado (c/ gist e GitHub Pages)_\\n\\n_Google Cloud Product Fundamentals_\\n\\nOutros cursos e mais informações estão disponíveis no _LinkedIn_.\\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 238
        }
      ]
    }
  ]
}